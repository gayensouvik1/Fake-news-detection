{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "for i in range (0,6):\n",
    "    temp = 'news/news_1/news_1_'+ str(i) + '.txt'\n",
    "    with open(temp, 'r') as myfile:\n",
    "        data.append(myfile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re,nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_words( raw_review ):\n",
    "# 2. Remove non-letters        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", raw_review) \n",
    "    #\n",
    "    # 3. Convert to lower case, split into individual words\n",
    "    words = letters_only.lower().split()                             \n",
    "    #\n",
    "    # 4. In Python, searching a set is much faster than searching\n",
    "    #   a list, so convert the stop words to a set\n",
    "    stops = set(stopwords.words(\"english\"))                  \n",
    "    # \n",
    "    # 5. Remove stop words\n",
    "    meaningful_words = [w for w in words if not w in stops]   \n",
    "    #\n",
    "    # 6. Join the words back into one string separated by space, \n",
    "    # and return the result.\n",
    "    return( \" \".join( meaningful_words ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_train_reviews = []\n",
    "for i in range (0,6):\n",
    "    clean_train_reviews.append(review_to_words(data[i]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the bag of words...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Creating the bag of words...\\n\"\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
    "# bag of words tool.  \n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) \n",
    "\n",
    "# fit_transform() does two functions: First, it fits the model\n",
    "# and learns the vocabulary; second, it transforms our training data\n",
    "# into feature vectors. The input to fit_transform should be a list of \n",
    "# strings.\n",
    "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Numpy arrays are easy to work with, so convert the result to an \n",
    "# array\n",
    "train_data_features = train_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 3, 1, 1, 0, 4, 0, 1, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       2, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 3, 0, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 5, 0, 0,\n",
       "       1, 0, 2, 1, 0, 0, 1, 2, 0, 0, 1, 0, 0, 0, 0, 0, 2, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 1,\n",
       "       0, 1, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 2, 0, 1, 1, 0, 1, 1, 0, 1, 3, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 2, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 5, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       1, 2, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 2, 1, 1, 0, 1, 0, 2, 0, 1, 1, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 7, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 3, 2, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_features[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'abc', u'aborted', u'accentuates', u'accident', u'accidents', u'according', u'account', u'accusations', u'accused', u'acquisition', u'action', u'added', u'addition', u'adhere', u'admit', u'advanced', u'affiliate', u'aftermath', u'aggressive', u'ago', u'agreed', u'airport', u'allegedly', u'alleging', u'allow', u'alphabet', u'also', u'altercation', u'although', u'always', u'ambitious', u'announced', u'another', u'anthony', u'apache', u'apiece', u'apologize', u'app', u'appears', u'applied', u'approach', u'ariz', u'arizona', u'atg', u'august', u'authorities', u'auto', u'autonomous', u'back', u'backseat', u'bad', u'badly', u'ban', u'banned', u'based', u'became', u'began', u'begin', u'behavior', u'behind', u'berating', u'beyond', u'blog', u'bloomberg', u'boulevard', u'breezing', u'broken', u'building', u'business', u'california', u'called', u'cam', u'came', u'canyon', u'captured', u'car', u'carnegie', u'carrying', u'cars', u'caused', u'caution', u'ceo', u'certainly', u'change', u'changing', u'chelsea', u'chief', u'cities', u'city', u'clear', u'co', u'collision', u'comes', u'companies', u'company', u'complaints', u'comply', u'concerns', u'confirm', u'confirmed', u'contentious', u'continue', u'continued', u'continuing', u'control', u'controlling', u'controls', u'corporate', u'country', u'cracked', u'cracking', u'crash', u'crises', u'crosswalk', u'crunch', u'culpability', u'culture', u'currently', u'damaged', u'dash', u'day', u'december', u'decided', u'decision', u'decisions', u'deeper', u'deleteuber', u'demonstrations', u'dented', u'department', u'deploying', u'described', u'designs', u'despite', u'details', u'development', u'difficult', u'difficulty', u'disclose', u'dispute', u'disturbed', u'documents', u'dodge', u'done', u'doubts', u'doug', u'drive', u'driver', u'drivers', u'driving', u'ducey', u'duration', u'earlier', u'early', u'economic', u'employees', u'ended', u'enforcement', u'engineer', u'engineering', u'engineers', u'entire', u'err', u'error', u'ethics', u'evening', u'executive', u'executives', u'existence', u'expand', u'explain', u'face', u'failed', u'failure', u'faith', u'far', u'fault', u'february', u'female', u'fielded', u'first', u'fleet', u'flip', u'flipped', u'followed', u'following', u'follows', u'forced', u'ford', u'former', u'founder', u'four', u'francisco', u'friday', u'fundamentally', u'general', u'given', u'go', u'gone', u'google', u'governor', u'grand', u'granted', u'greyball', u'ground', u'grounded', u'grounding', u'group', u'grow', u'hailing', u'halt', u'happened', u'harassment', u'hard', u'head', u'hit', u'holding', u'hours', u'however', u'human', u'humans', u'hurt', u'immediate', u'impatience', u'improvement', u'incident', u'include', u'included', u'including', u'increase', u'individual', u'injured', u'injuries', u'inside', u'instance', u'instead', u'internal', u'introduction', u'investigates', u'investigation', u'involved', u'involving', u'january', u'jeff', u'job', u'jones', u'jose', u'josie', u'kalanick', u'kohler', u'lack', u'last', u'late', u'latest', u'leader', u'leadership', u'least', u'led', u'left', u'legal', u'less', u'levandowski', u'lights', u'limelight', u'litany', u'live', u'local', u'locations', u'long', u'longer', u'look', u'looking', u'make', u'making', u'march', u'mass', u'may', u'mcclintock', u'mellon', u'mention', u'might', u'mission', u'mode', u'montenegro', u'month', u'months', u'motor', u'motors', u'move', u'ms', u'multiple', u'must', u'nature', u'near', u'necessary', u'negative', u'negotiating', u'neither', u'new', u'news', u'next', u'noted', u'number', u'obstacles', u'obtain', u'occurred', u'offer', u'offered', u'officers', u'officials', u'one', u'online', u'onto', u'operate', u'operated', u'operating', u'operation', u'option', u'original', u'otto', u'overturned', u'parked', u'part', u'passenger', u'passengers', u'past', u'pending', u'pennsylvania', u'permit', u'permits', u'permitting', u'person', u'persuade', u'phoenix', u'photo', u'pictures', u'pilot', u'piloting', u'pittsburg', u'pittsburgh', u'poaching', u'police', u'position', u'possibly', u'posted', u'practices', u'president', u'presumably', u'pricing', u'problem', u'problematic', u'program', u'progress', u'project', u'projects', u'prompting', u'prone', u'proper', u'prospects', u'protested', u'public', u'publicity', u'published', u'pulled', u'pushed', u'put', u'quickly', u'quit', u'ran', u'recent', u'recently', u'recode', u'recognize', u'red', u'regions', u'registered', u'registering', u'registrations', u'regular', u'regulation', u'regulators', u'related', u'released', u'remain', u'reported', u'reportedly', u'reports', u'requesting', u'require', u'required', u'resigned', u'responsible', u'rest', u'result', u'results', u'return', u'returned', u'reviewed', u'revoked', u'ride', u'right', u'road', u'roads', u'roll', u'rolled', u'rolling', u'rollout', u'rooted', u'rules', u'run', u'said', u'sailed', u'san', u'saturday', u'say', u'saying', u'seat', u'seek', u'seen', u'self', u'sensor', u'serious', u'seriously', u'service', u'set', u'setbacks', u'several', u'sexism', u'sexual', u'sf', u'shakeup', u'sharing', u'shortly', u'showed', u'showing', u'shut', u'side', u'since', u'sites', u'sitting', u'six', u'skirt', u'slammed', u'smaller', u'smarter', u'software', u'special', u'spending', u'spokesman', u'spokesperson', u'spokeswoman', u'sport', u'staffers', u'start', u'startup', u'state', u'statement', u'states', u'station', u'stealing', u'stoplight', u'stoplights', u'stories', u'stranger', u'strategic', u'street', u'string', u'struck', u'struggles', u'successful', u'sued', u'suffered', u'suggesting', u'surge', u'suspend', u'suspended', u'suspending', u'suspension', u'suv', u'suvs', u'systemic', u'systems', u'take', u'taken', u'talent', u'taxi', u'tech', u'technically', u'technology', u'tempe', u'test', u'tested', u'testers', u'testing', u'theory', u'things', u'third', u'thoroughly', u'threatened', u'three', u'time', u'times', u'timetable', u'timing', u'today', u'told', u'took', u'tool', u'top', u'travel', u'travis', u'tried', u'trucks', u'trump', u'tumultuous', u'turn', u'turned', u'turns', u'twitter', u'two', u'typically', u'uber', u'ubers', u'uberx', u'uncertain', u'undertakes', u'unhappy', u'university', u'unknown', u'us', u'use', u'used', u'users', u'utility', u'variable', u'vehicle', u'vehicles', u'verbal', u'video', u'viral', u'volvo', u'want', u'way', u'waymo', u'week', u'weeks', u'welcomed', u'well', u'went', u'wheel', u'whether', u'wider', u'windows', u'without', u'work', u'working', u'workplace', u'worldwide', u'would', u'wrong', u'wsj', u'xc', u'year', u'years', u'yield', u'york']\n"
     ]
    }
   ],
   "source": [
    "# Take a look at the words in the vocabulary\n",
    "vocab = vectorizer.get_feature_names()\n",
    "print vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 abc\n",
      "1 aborted\n",
      "1 accentuates\n",
      "13 accident\n",
      "3 accidents\n",
      "6 according\n",
      "1 account\n",
      "1 accusations\n",
      "1 accused\n",
      "1 acquisition\n",
      "2 action\n",
      "1 added\n",
      "1 addition\n",
      "1 adhere\n",
      "1 admit\n",
      "1 advanced\n",
      "1 affiliate\n",
      "1 aftermath\n",
      "1 aggressive\n",
      "1 ago\n",
      "1 agreed\n",
      "1 airport\n",
      "2 allegedly\n",
      "5 alleging\n",
      "1 allow\n",
      "1 alphabet\n",
      "1 also\n",
      "1 altercation\n",
      "1 although\n",
      "7 always\n",
      "1 ambitious\n",
      "1 announced\n",
      "1 another\n",
      "1 anthony\n",
      "1 apache\n",
      "1 apiece\n",
      "2 apologize\n",
      "15 app\n",
      "3 appears\n",
      "1 applied\n",
      "2 approach\n",
      "1 ariz\n",
      "17 arizona\n",
      "3 atg\n",
      "2 august\n",
      "1 authorities\n",
      "1 auto\n",
      "1 autonomous\n",
      "1 back\n",
      "2 backseat\n",
      "1 bad\n",
      "5 badly\n",
      "1 ban\n",
      "1 banned\n",
      "5 based\n",
      "1 became\n",
      "1 began\n",
      "3 begin\n",
      "1 behavior\n",
      "1 behind\n",
      "1 berating\n",
      "1 beyond\n",
      "1 blog\n",
      "13 bloomberg\n",
      "1 boulevard\n",
      "1 breezing\n",
      "1 broken\n",
      "1 building\n",
      "1 business\n",
      "22 california\n",
      "1 called\n",
      "1 cam\n",
      "13 came\n",
      "2 canyon\n",
      "1 captured\n",
      "1 car\n",
      "1 carnegie\n",
      "1 carrying\n",
      "1 cars\n",
      "1 caused\n",
      "1 caution\n",
      "2 ceo\n",
      "2 certainly\n",
      "5 change\n",
      "2 changing\n",
      "2 chelsea\n",
      "20 chief\n",
      "1 cities\n",
      "1 city\n",
      "1 clear\n",
      "1 co\n",
      "3 collision\n",
      "1 comes\n",
      "1 companies\n",
      "1 company\n",
      "3 complaints\n",
      "2 comply\n",
      "1 concerns\n",
      "1 confirm\n",
      "1 confirmed\n",
      "1 contentious\n",
      "13 continue\n",
      "1 continued\n",
      "1 continuing\n",
      "1 control\n",
      "1 controlling\n",
      "3 controls\n",
      "1 corporate\n",
      "1 country\n",
      "2 cracked\n",
      "1 cracking\n",
      "1 crash\n",
      "1 crises\n",
      "1 crosswalk\n",
      "1 crunch\n",
      "1 culpability\n",
      "3 culture\n",
      "1 currently\n",
      "1 damaged\n",
      "1 dash\n",
      "2 day\n",
      "1 december\n",
      "2 decided\n",
      "1 decision\n",
      "1 decisions\n",
      "1 deeper\n",
      "1 deleteuber\n",
      "1 demonstrations\n",
      "1 dented\n",
      "1 department\n",
      "1 deploying\n",
      "1 described\n",
      "1 designs\n",
      "9 despite\n",
      "4 details\n",
      "32 development\n",
      "1 difficult\n",
      "5 difficulty\n",
      "1 disclose\n",
      "1 dispute\n",
      "2 disturbed\n",
      "1 documents\n",
      "1 dodge\n",
      "1 done\n",
      "1 doubts\n",
      "3 doug\n",
      "1 drive\n",
      "1 driver\n",
      "2 drivers\n",
      "2 driving\n",
      "1 ducey\n",
      "1 duration\n",
      "1 earlier\n",
      "1 early\n",
      "9 economic\n",
      "1 employees\n",
      "1 ended\n",
      "1 enforcement\n",
      "6 engineer\n",
      "1 engineering\n",
      "1 engineers\n",
      "1 entire\n",
      "2 err\n",
      "1 error\n",
      "1 ethics\n",
      "1 evening\n",
      "1 executive\n",
      "1 executives\n",
      "2 existence\n",
      "1 expand\n",
      "2 explain\n",
      "2 face\n",
      "8 failed\n",
      "6 failure\n",
      "1 faith\n",
      "1 far\n",
      "1 fault\n",
      "1 february\n",
      "1 female\n",
      "3 fielded\n",
      "2 first\n",
      "1 fleet\n",
      "2 flip\n",
      "1 flipped\n",
      "1 followed\n",
      "1 following\n",
      "2 follows\n",
      "1 forced\n",
      "2 ford\n",
      "1 former\n",
      "1 founder\n",
      "3 four\n",
      "2 francisco\n",
      "1 friday\n",
      "1 fundamentally\n",
      "3 general\n",
      "1 given\n",
      "2 go\n",
      "1 gone\n",
      "1 google\n",
      "8 governor\n",
      "1 grand\n",
      "3 granted\n",
      "1 greyball\n",
      "1 ground\n",
      "1 grounded\n",
      "3 grounding\n",
      "1 group\n",
      "2 grow\n",
      "1 hailing\n",
      "2 halt\n",
      "3 happened\n",
      "5 harassment\n",
      "1 hard\n",
      "1 head\n",
      "3 hit\n",
      "1 holding\n",
      "3 hours\n",
      "1 however\n",
      "2 human\n",
      "3 humans\n",
      "3 hurt\n",
      "1 immediate\n",
      "5 impatience\n",
      "1 improvement\n",
      "1 incident\n",
      "1 include\n",
      "1 included\n",
      "2 including\n",
      "4 increase\n",
      "1 individual\n",
      "2 injured\n",
      "2 injuries\n",
      "1 inside\n",
      "1 instance\n",
      "1 instead\n",
      "1 internal\n",
      "1 introduction\n",
      "1 investigates\n",
      "1 investigation\n",
      "1 involved\n",
      "1 involving\n",
      "1 january\n",
      "1 jeff\n",
      "1 job\n",
      "1 jones\n",
      "2 jose\n",
      "1 josie\n",
      "1 kalanick\n",
      "1 kohler\n",
      "5 lack\n",
      "4 last\n",
      "3 late\n",
      "4 latest\n",
      "3 leader\n",
      "1 leadership\n",
      "2 least\n",
      "3 led\n",
      "1 left\n",
      "3 legal\n",
      "1 less\n",
      "1 levandowski\n",
      "1 lights\n",
      "1 limelight\n",
      "1 litany\n",
      "1 live\n",
      "4 local\n",
      "3 locations\n",
      "2 long\n",
      "3 longer\n",
      "1 look\n",
      "1 looking\n",
      "3 make\n",
      "1 making\n",
      "1 march\n",
      "1 mass\n",
      "1 may\n",
      "11 mcclintock\n",
      "1 mellon\n",
      "3 mention\n",
      "3 might\n",
      "1 mission\n",
      "2 mode\n",
      "1 montenegro\n",
      "1 month\n",
      "1 months\n",
      "3 motor\n",
      "1 motors\n",
      "1 move\n",
      "1 ms\n",
      "3 multiple\n",
      "2 must\n",
      "1 nature\n",
      "1 near\n",
      "6 necessary\n",
      "1 negative\n",
      "4 negotiating\n",
      "1 neither\n",
      "1 new\n",
      "1 news\n",
      "3 next\n",
      "4 noted\n",
      "1 number\n",
      "7 obstacles\n",
      "1 obtain\n",
      "1 occurred\n",
      "2 offer\n",
      "2 offered\n",
      "4 officers\n",
      "1 officials\n",
      "1 one\n",
      "7 online\n",
      "1 onto\n",
      "2 operate\n",
      "1 operated\n",
      "1 operating\n",
      "1 operation\n",
      "1 option\n",
      "1 original\n",
      "1 otto\n",
      "2 overturned\n",
      "1 parked\n",
      "3 part\n",
      "1 passenger\n",
      "1 passengers\n",
      "2 past\n",
      "2 pending\n",
      "1 pennsylvania\n",
      "2 permit\n",
      "2 permits\n",
      "2 permitting\n",
      "2 person\n",
      "1 persuade\n",
      "1 phoenix\n",
      "1 photo\n",
      "2 pictures\n",
      "1 pilot\n",
      "1 piloting\n",
      "2 pittsburg\n",
      "5 pittsburgh\n",
      "1 poaching\n",
      "1 police\n",
      "1 position\n",
      "2 possibly\n",
      "2 posted\n",
      "1 practices\n",
      "1 president\n",
      "1 presumably\n",
      "1 pricing\n",
      "1 problem\n",
      "1 problematic\n",
      "2 program\n",
      "2 progress\n",
      "2 project\n",
      "4 projects\n",
      "2 prompting\n",
      "1 prone\n",
      "1 proper\n",
      "1 prospects\n",
      "1 protested\n",
      "1 public\n",
      "2 publicity\n",
      "16 published\n",
      "1 pulled\n",
      "8 pushed\n",
      "5 put\n",
      "1 quickly\n",
      "1 quit\n",
      "5 ran\n",
      "1 recent\n",
      "1 recently\n",
      "30 recode\n",
      "1 recognize\n",
      "4 red\n",
      "1 regions\n",
      "1 registered\n",
      "1 registering\n",
      "1 registrations\n",
      "2 regular\n",
      "1 regulation\n",
      "1 regulators\n",
      "1 related\n",
      "1 released\n",
      "1 remain\n",
      "1 reported\n",
      "1 reportedly\n",
      "7 reports\n",
      "2 requesting\n",
      "1 require\n",
      "1 required\n",
      "1 resigned\n",
      "1 responsible\n",
      "1 rest\n",
      "1 result\n",
      "1 results\n",
      "1 return\n",
      "1 returned\n",
      "2 reviewed\n",
      "5 revoked\n",
      "2 ride\n",
      "1 right\n",
      "1 road\n",
      "2 roads\n",
      "4 roll\n",
      "3 rolled\n",
      "2 rolling\n",
      "1 rollout\n",
      "1 rooted\n",
      "1 rules\n",
      "1 run\n",
      "1 said\n",
      "1 sailed\n",
      "1 san\n",
      "1 saturday\n",
      "3 say\n",
      "2 saying\n",
      "1 seat\n",
      "1 seek\n",
      "1 seen\n",
      "1 self\n",
      "1 sensor\n",
      "1 serious\n",
      "3 seriously\n",
      "4 service\n",
      "8 set\n",
      "1 setbacks\n",
      "2 several\n",
      "4 sexism\n",
      "1 sexual\n",
      "2 sf\n",
      "2 shakeup\n",
      "1 sharing\n",
      "4 shortly\n",
      "9 showed\n",
      "2 showing\n",
      "3 shut\n",
      "1 side\n",
      "14 since\n",
      "1 sites\n",
      "1 sitting\n",
      "1 six\n",
      "1 skirt\n",
      "1 slammed\n",
      "1 smaller\n",
      "12 smarter\n",
      "3 software\n",
      "1 special\n",
      "3 spending\n",
      "1 spokesman\n",
      "2 spokesperson\n",
      "1 spokeswoman\n",
      "1 sport\n",
      "2 staffers\n",
      "2 start\n",
      "1 startup\n",
      "1 state\n",
      "1 statement\n",
      "2 states\n",
      "1 station\n",
      "1 stealing\n",
      "1 stoplight\n",
      "2 stoplights\n",
      "1 stories\n",
      "60 stranger\n",
      "1 strategic\n",
      "2 street\n",
      "1 string\n",
      "1 struck\n",
      "1 struggles\n",
      "1 successful\n",
      "1 sued\n",
      "2 suffered\n",
      "1 suggesting\n",
      "2 surge\n",
      "1 suspend\n",
      "22 suspended\n",
      "23 suspending\n",
      "1 suspension\n",
      "1 suv\n",
      "1 suvs\n",
      "4 systemic\n",
      "1 systems\n",
      "3 take\n",
      "1 taken\n",
      "2 talent\n",
      "3 taxi\n",
      "1 tech\n",
      "1 technically\n",
      "5 technology\n",
      "2 tempe\n",
      "1 test\n",
      "3 tested\n",
      "1 testers\n",
      "2 testing\n",
      "3 theory\n",
      "1 things\n",
      "1 third\n",
      "1 thoroughly\n",
      "2 threatened\n",
      "4 three\n",
      "1 time\n",
      "6 times\n",
      "3 timetable\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sum up the counts of each vocabulary word\n",
    "dist = np.sum(train_data_features, axis=0)\n",
    "\n",
    "# For each, print the vocabulary word and the number of times it \n",
    "# appears in the training set\n",
    "for tag, count in zip(vocab, dist):\n",
    "    print count, tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the random forest...\n"
     ]
    }
   ],
   "source": [
    "print \"Training the random forest...\"\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize a Random Forest classifier with 100 trees\n",
    "forest = RandomForestClassifier(n_estimators = 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit the forest to the training set, using the bag of words as \n",
    "# features and the sentiment labels as the response variable\n",
    "#\n",
    "# This may take a few minutes to run\n",
    "y = [1,1,1,1,1,1]\n",
    "forest = forest.fit( train_data_features ,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test = []\n",
    "with open(\"news/news_1/news_1_8.txt\", 'r') as myfile:\n",
    "    test.append(myfile.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_test_review = review_to_words( test[0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_test_reviews = []\n",
    "clean_test_reviews.append( clean_test_review )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['luis enrique leaving nou camp end season barca star man appears taking hold matters proper investment required team following poor signings made last year enrique recruits andre gomes lucas digne paco alcacer failed impress spanish giants despite big price tags lionel messi apparently believes younger trio sufficient standard barcelona']"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_test_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get a bag of words for the test set, and convert to a numpy array\n",
    "test_data_features = vectorizer.transform(clean_test_reviews)\n",
    "test_data_features = test_data_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Use the random forest to make sentiment label predictions\n",
    "result = forest.predict(test_data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
